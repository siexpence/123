# # import torch
# # import random
# # import numpy as np
# # import logging
# # from torch_geometric.datasets import EllipticBitcoinDataset
# # import torch.nn as nn
# # import torch.nn.functional as F
# # from torch_geometric.nn import GATConv
# # from sklearn.preprocessing import StandardScaler
# # from sklearn.metrics import f1_score, precision_score, recall_score
# # import pandas as pd
# # import matplotlib.pyplot as plt
# #
# # # 设置随机种子
# # seed = 42
# # torch.manual_seed(seed)
# # np.random.seed(seed)
# # random.seed(seed)
# # if torch.cuda.is_available():
# #     torch.cuda.manual_seed_all(seed)
# #
# # # 日志配置
# # logging.basicConfig(filename='tgat_training.log', level=logging.INFO,
# #                     format='%(asctime)s - %(levelname)s - %(message)s')
# #
# # # 超参数配置
# # config = {
# #     'in_channels': 165,
# #     'hidden_channels': 64,
# #     'out_channels': 2,
# #     'lr': 0.01,
# #     'weight_decay': 5e-4,
# #     'epochs': 200,
# #     'patience': 20,
# #     'dropout': 0.3,
# #     'num_heads': 4,
# # }
# #
# #
# # # 自定义时间编码
# # class TemporalEncoding(nn.Module):
# #     def __init__(self, d_model):
# #         super(TemporalEncoding, self).__init__()
# #         self.d_model = d_model
# #         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
# #         self.register_buffer('div_term', div_term)
# #
# #     def forward(self, t):
# #         pe = torch.zeros(t.size(0), self.d_model, device=t.device)
# #         t = t.unsqueeze(-1)
# #         pe[:, 0::2] = torch.sin(t * self.div_term)
# #         pe[:, 1::2] = torch.cos(t * self.div_term)
# #         return pe
# #
# #
# # # 自定义 TGAT 卷积层
# # class CustomTemporalGATConv(nn.Module):
# #     def __init__(self, in_channels, out_channels, heads=1, dropout=0.0):
# #         super(CustomTemporalGATConv, self).__init__()
# #         self.temporal_encoder = TemporalEncoding(out_channels)
# #         self.gat = GATConv(in_channels + out_channels, out_channels, heads=heads, dropout=dropout)
# #         self.dropout = dropout
# #
# #     def forward(self, x, edge_index, edge_time):
# #         time_emb = self.temporal_encoder(edge_time)
# #         edge_time_emb = time_emb.repeat(1, x.size(0)).view(x.size(0), -1)
# #         x = torch.cat([x, edge_time_emb], dim=-1)
# #         x = self.gat(x, edge_index)
# #         return x
# #
# #
# # # 加载数据集
# # dataset = EllipticBitcoinDataset(root='data/EllipticBitcoinDataset')
# # data = dataset[0]
# #
# # # 设备
# # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# # logging.info(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
# # data = data.to(device)
# #
# # # 检查数据集完整性
# # if data.x is None or data.edge_index is None or data.y is None:
# #     raise ValueError("Invalid dataset: missing features, edges, or labels")
# #
# # # 特征归一化
# # scaler = StandardScaler()
# # data.x = torch.tensor(scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)
# #
# # # 处理时间戳（假设 edge_attr 第一列为时间戳）
# # if hasattr(data, 'edge_attr') and data.edge_attr is not None:
# #     edge_time = data.edge_attr[:, 0].clone().to(device)
# #     edge_time = (edge_time - edge_time.min()) / (edge_time.max() - edge_time.min() + 1e-6)
# # else:
# #     edge_time = torch.arange(data.edge_index.shape[1], dtype=torch.float, device=device)
# #     edge_time = edge_time / (edge_time.max() + 1e-6)
# # logging.info(f"Edge time shape: {edge_time.shape}")
# #
# # # 数据分布检查
# # known_indices = (data.y != 2).nonzero(as_tuple=True)[0]
# # num_known_nodes = known_indices.shape[0]
# # logging.info(f"Number of known nodes: {num_known_nodes}")
# # print(f"Number of known nodes: {num_known_nodes}")
# # print("Label distribution in known nodes:", data.y[known_indices].bincount().cpu().numpy())
# #
# # # 训练、验证、测试分割
# # indices = torch.randperm(num_known_nodes)
# # train_size = int(num_known_nodes * 0.6)
# # val_size = int(num_known_nodes * 0.2)
# # train_indices = known_indices[indices[:train_size]]
# # val_indices = known_indices[indices[train_size:train_size + val_size]]
# # test_indices = known_indices[indices[train_size + val_size:]]
# #
# # train_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
# # val_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
# # test_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
# # train_mask[train_indices] = True
# # val_mask[val_indices] = True
# # test_mask[test_indices] = True
# # logging.info(
# #     f"Train mask shape: {train_mask.shape}, Val mask shape: {val_mask.shape}, Test mask shape: {test_mask.shape}")
# #
# #
# # # 定义 TGAT 模型
# # class TGAT(torch.nn.Module):
# #     def __init__(self, in_channels, hidden_channels, out_channels, num_heads, dropout):
# #         super(TGAT, self).__init__()
# #         self.conv1 = CustomTemporalGATConv(in_channels, hidden_channels, heads=num_heads, dropout=dropout)
# #         self.bn1 = torch.nn.BatchNorm1d(hidden_channels * num_heads)
# #         self.conv2 = CustomTemporalGATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads,
# #                                            dropout=dropout)
# #         self.bn2 = torch.nn.BatchNorm1d(hidden_channels * num_heads)
# #         self.conv3 = CustomTemporalGATConv(hidden_channels * num_heads, out_channels, heads=1, dropout=dropout)
# #         self.dropout = torch.nn.Dropout(dropout)
# #
# #     def forward(self, x, edge_index, edge_time):
# #         x = self.conv1(x, edge_index, edge_time)
# #         x = self.bn1(x)
# #         x = F.relu(x)
# #         x = F.dropout(x, p=self.dropout, training=self.training)
# #         x = self.conv2(x, edge_index, edge_time)
# #         x = self.bn2(x)
# #         x = F.relu(x)
# #         x = F.dropout(x, p=self.dropout, training=self.training)
# #         x = self.conv3(x, edge_index, edge_time)
# #         return F.log_softmax(x, dim=1)
# #
# #
# # # 初始化模型和优化器
# # model = TGAT(
# #     in_channels=config['in_channels'],
# #     hidden_channels=config['hidden_channels'],
# #     out_channels=config['out_channels'],
# #     num_heads=config['num_heads'],
# #     dropout=config['dropout']
# # ).to(device)
# # optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
# #
# #
# # # 训练和评估函数
# # def train():
# #     model.train()
# #     optimizer.zero_grad()
# #     out = model(data.x, data.edge_index, edge_time)
# #     loss = F.nll_loss(out[train_mask], data.y[train_mask])
# #     loss.backward()
# #     optimizer.step()
# #     return loss.item()
# #
# #
# # def evaluate(mask):
# #     model.eval()
# #     with torch.no_grad():
# #         out = model(data.x, data.edge_index, edge_time)
# #         pred = out.argmax(dim=1)
# #         correct = (pred[mask] == data.y[mask]).sum()
# #         acc = correct.float() / mask.sum().float()
# #         pred_cpu = pred[mask].cpu().numpy()
# #         true_cpu = data.y[mask].cpu().numpy()
# #         try:
# #             f1 = f1_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
# #             precision = precision_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
# #             recall = recall_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
# #         except Exception as e:
# #             logging.warning(f"Evaluation metrics warning: {e}")
# #             f1, precision, recall = 0.0, 0.0, 0.0
# #     return acc.item(), f1, precision, recall
# #
# #
# # # 训练循环
# # best_val_acc = 0
# # counter = 0
# # train_losses, val_accs, test_accs = [], [], []
# #
# # for epoch in range(config['epochs']):
# #     loss = train()
# #     val_acc, val_f1, val_precision, val_recall = evaluate(val_mask)
# #     scheduler.step(loss)
# #
# #     train_losses.append(loss)
# #     val_accs.append(val_acc)
# #
# #     if epoch % 10 == 0:
# #         test_acc, test_f1, test_precision, test_recall = evaluate(test_mask)
# #         test_accs.append(test_acc)
# #         logging.info(f'Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, '
# #                      f'F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
# #         print(f'Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, '
# #               f'F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
# #
# #     if val_acc > best_val_acc:
# #         best_val_acc = val_acc
# #         counter = 0
# #         torch.save(model.state_dict(), 'best_tgat_model.pth')
# #     else:
# #         counter += 1
# #         if counter >= config['patience']:
# #             logging.info("Early stopping triggered")
# #             print("Early stopping triggered")
# #             break
# #
# # # 加载最佳模型
# # model.load_state_dict(torch.load('best_tgat_model.pth'))
# #
# # # 最终评估
# # test_acc, test_f1, test_precision, test_recall = evaluate(test_mask)
# # logging.info(
# #     f'Final Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
# # print(f'Final Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
# #
# # # 推断未知节点
# # model.eval()
# # with torch.no_grad():
# #     out = model(data.x, data.edge_index, edge_time)
# #     pred = out.argmax(dim=1)
# #     probs = F.softmax(out, dim=1)
# #     unknown_mask = (data.y == 2)
# #     unknown_indices = unknown_mask.nonzero(as_tuple=True)[0]
# #     unknown_preds = pred[unknown_mask]
# #     unknown_probs = probs[unknown_mask]
# #
# #     # 预测分布可视化
# #     plt.hist(unknown_preds.cpu().numpy(), bins=2, edgecolor='black')
# #     plt.title("Prediction Distribution for Unknown Nodes")
# #     plt.xlabel("Predicted Label")
# #     plt.ylabel("Count")
# #     plt.savefig('unknown_pred_distribution.png')
# #     plt.close()
# #
# #     # 保存结果
# #     result_df = pd.DataFrame({
# #         'NodeIndex': unknown_indices.cpu().numpy(),
# #         'PredictedLabel': unknown_preds.cpu().numpy(),
# #         'Prob_0': unknown_probs[:, 0].cpu().numpy(),
# #         'Prob_1': unknown_probs[:, 1].cpu().numpy()
# #     })
# #     result_df.to_csv('unknown_predictions_tgat.csv', index=False)
# #     logging.info("Predictions saved to 'unknown_predictions_tgat.csv'")
# #     print("Predictions saved to 'unknown_predictions_tgat.csv'")
# #
# # # 绘制学习曲线
# # plt.plot(train_losses, label='Train Loss')
# # plt.plot(val_accs, label='Val Accuracy')
# # plt.plot([i * 10 for i in range(len(test_accs))], test_accs, label='Test Accuracy')
# # plt.legend()
# # plt.title("Learning Curves")
# # plt.xlabel("Epoch")
# # plt.ylabel("Value")
# # plt.savefig('learning_curve_tgat.png')
# # plt.close()
# #
# # # 检查预测分布
# # print("Known nodes label distribution:", data.y[train_mask | test_mask].bincount().cpu().numpy())
# # print("Unknown nodes prediction distribution:", unknown_preds.bincount().cpu().numpy())
# #
#
#
# import torch
# import random
# import numpy as np
# import logging
# from torch_geometric.datasets import EllipticBitcoinDataset
# import torch.nn as nn
# import torch.nn.functional as F
# from torch_geometric.nn import GATConv
# from sklearn.preprocessing import StandardScaler
# from sklearn.metrics import f1_score, precision_score, recall_score
# import pandas as pd
# import matplotlib.pyplot as plt
#
# # Set random seed
# seed = 42
# torch.manual_seed(seed)
# np.random.seed(seed)
# random.seed(seed)
# if torch.cuda.is_available():
#     torch.cuda.manual_seed_all(seed)
#
# # Logging configuration
# logging.basicConfig(filename='tgat_training.log', level=logging.INFO,
#                     format='%(asctime)s - %(levelname)s - %(message)s')
#
# # Hyperparameters
# config = {
#     'in_channels': 165,
#     'hidden_channels': 64,
#     'out_channels': 2,
#     'lr': 0.01,
#     'weight_decay': 5e-4,
#     'epochs': 200,
#     'patience': 20,
#     'dropout': 0.3,
#     'num_heads': 4,
# }
#
# # Temporal Encoding
# class TemporalEncoding(nn.Module):
#     def __init__(self, d_model):
#         super(TemporalEncoding, self).__init__()
#         self.d_model = d_model
#         div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
#         self.register_buffer('div_term', div_term)
#
#     def forward(self, t):
#         pe = torch.zeros(t.size(0), self.d_model, device=t.device)
#         t = t.unsqueeze(-1)
#         pe[:, 0::2] = torch.sin(t * self.div_term)
#         pe[:, 1::2] = torch.cos(t * self.div_term)
#         return pe
#
# # Custom Temporal GAT Convolution
# class CustomTemporalGATConv(nn.Module):
#     def __init__(self, in_channels, out_channels, heads=1, dropout=0.0):
#         super(CustomTemporalGATConv, self).__init__()
#         self.temporal_encoder = TemporalEncoding(out_channels)
#         self.gat = GATConv(in_channels, out_channels, heads=heads, dropout=dropout)
#         self.dropout = dropout
#
#     def forward(self, x, edge_index, edge_time):
#         # Generate temporal embeddings for edges
#         time_emb = self.temporal_encoder(edge_time)  # Shape: [num_edges, out_channels]
#         num_nodes = x.size(0)
#         edge_time_emb = torch.zeros(num_nodes, time_emb.size(1), device=x.device)
#         row, col = edge_index  # Source and target nodes
#
#         # Aggregate temporal embeddings to nodes (mean aggregation)
#         for i in range(num_nodes):
#             mask = (col == i)
#             if mask.sum() > 0:
#                 edge_time_emb[i] = time_emb[mask].mean(dim=0)
#
#         # Concatenate node features with aggregated temporal embeddings
#         x = torch.cat([x, edge_time_emb], dim=-1)
#         x = self.gat(x, edge_index)
#         return x
#
# # Load dataset
# dataset = EllipticBitcoinDataset(root='data/EllipticBitcoinDataset')
# data = dataset[0]
#
# # Device
# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# logging.info(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
# data = data.to(device)
#
# # Check dataset integrity
# if data.x is None or data.edge_index is None or data.y is None:
#     raise ValueError("Invalid dataset: missing features, edges, or labels")
#
# # Feature normalization
# scaler = StandardScaler()
# data.x = torch.tensor(scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)
#
# # Process timestamps
# if hasattr(data, 'edge_attr') and data.edge_attr is not None:
#     edge_time = data.edge_attr[:, 0].clone().to(device)
#     edge_time = (edge_time - edge_time.min()) / (edge_time.max() - edge_time.min() + 1e-6)
# else:
#     edge_time = torch.arange(data.edge_index.shape[1], dtype=torch.float, device=device)
#     edge_time = edge_time / (edge_time.max() + 1e-6)
# logging.info(f"Edge time shape: {edge_time.shape}")
#
# # Data distribution
# known_indices = (data.y != 2).nonzero(as_tuple=True)[0]
# num_known_nodes = known_indices.shape[0]
# logging.info(f"Number of known nodes: {num_known_nodes}")
# print(f"Number of known nodes: {num_known_nodes}")
# print("Label distribution in known nodes:", data.y[known_indices].bincount().cpu().numpy())
#
# # Train/Val/Test split
# indices = torch.randperm(num_known_nodes)
# train_size = int(num_known_nodes * 0.6)
# val_size = int(num_known_nodes * 0.2)
# train_indices = known_indices[indices[:train_size]]
# val_indices = known_indices[indices[train_size:train_size + val_size]]
# test_indices = known_indices[indices[train_size + val_size:]]
#
# train_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
# val_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
# test_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
# train_mask[train_indices] = True
# val_mask[val_indices] = True
# test_mask[test_indices] = True
# logging.info(
#     f"Train mask shape: {train_mask.shape}, Val mask shape: {val_mask.shape}, Test mask shape: {test_mask.shape}")
#
# # Define TGAT model
# class TGAT(torch.nn.Module):
#     def __init__(self, in_channels, hidden_channels, out_channels, num_heads, dropout):
#         super(TGAT, self).__init__()
#         self.conv1 = CustomTemporalGATConv(in_channels, hidden_channels, heads=num_heads, dropout=dropout)
#         self.bn1 = torch.nn.BatchNorm1d(hidden_channels * num_heads)
#         self.conv2 = CustomTemporalGATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=dropout)
#         self.bn2 = torch.nn.BatchNorm1d(hidden_channels * num_heads)
#         self.conv3 = CustomTemporalGATConv(hidden_channels * num_heads, out_channels, heads=1, dropout=dropout)
#         self.dropout = torch.nn.Dropout(dropout)
#
#     def forward(self, x, edge_index, edge_time):
#         x = self.conv1(x, edge_index, edge_time)
#         x = self.bn1(x)
#         x = F.relu(x)
#         x = F.dropout(x, p=self.dropout, training=self.training)
#         x = self.conv2(x, edge_index, edge_time)
#         x = self.bn2(x)
#         x = F.relu(x)
#         x = F.dropout(x, p=self.dropout, training=self.training)
#         x = self.conv3(x, edge_index, edge_time)
#         return F.log_softmax(x, dim=1)
#
# # Initialize model and optimizer
# model = TGAT(
#     in_channels=config['in_channels'],
#     hidden_channels=config['hidden_channels'],
#     out_channels=config['out_channels'],
#     num_heads=config['num_heads'],
#     dropout=config['dropout']
# ).to(device)
# optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)
#
# # Training and evaluation functions
# def train():
#     model.train()
#     optimizer.zero_grad()
#     out = model(data.x, data.edge_index, edge_time)
#     loss = F.nll_loss(out[train_mask], data.y[train_mask])
#     loss.backward()
#     optimizer.step()
#     return loss.item()
#
# def evaluate(mask):
#     model.eval()
#     with torch.no_grad():
#         out = model(data.x, data.edge_index, edge_time)
#         pred = out.argmax(dim=1)
#         correct = (pred[mask] == data.y[mask]).sum()
#         acc = correct.float() / mask.sum().float()
#         pred_cpu = pred[mask].cpu().numpy()
#         true_cpu = data.y[mask].cpu().numpy()
#         try:
#             f1 = f1_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
#             precision = precision_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
#             recall = recall_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
#         except Exception as e:
#             logging.warning(f"Evaluation metrics warning: {e}")
#             f1, precision, recall = 0.0, 0.0, 0.0
#     return acc.item(), f1, precision, recall
#
# # Training loop
# best_val_acc = 0
# counter = 0
# train_losses, val_accs, test_accs = [], [], []
#
# for epoch in range(config['epochs']):
#     loss = train()
#     val_acc, val_f1, val_precision, val_recall = evaluate(val_mask)
#     scheduler.step(loss)
#
#     train_losses.append(loss)
#     val_accs.append(val_acc)
#
#     if epoch % 10 == 0:
#         test_acc, test_f1, test_precision, test_recall = evaluate(test_mask)
#         test_accs.append(test_acc)
#         logging.info(f'Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, '
#                      f'F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
#         print(f'Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, '
#               f'F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
#
#     if val_acc > best_val_acc:
#         best_val_acc = val_acc
#         counter = 0
#         torch.save(model.state_dict(), 'best_tgat_model.pth')
#     else:
#         counter += 1
#         if counter >= config['patience']:
#             logging.info("Early stopping triggered")
#             print("Early stopping triggered")
#             break
#
# # Load best model
# model.load_state_dict(torch.load('best_tgat_model.pth'))
#
# # Final evaluation
# test_acc, test_f1, test_precision, test_recall = evaluate(test_mask)
# logging.info(
#     f'Final Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
# print(f'Final Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
#
# # Inference on unknown nodes
# model.eval()
# with torch.no_grad():
#     out = model(data.x, data.edge_index, edge_time)
#     pred = out.argmax(dim=1)
#     probs = F.softmax(out, dim=1)
#     unknown_mask = (data.y == 2)
#     unknown_indices = unknown_mask.nonzero(as_tuple=True)[0]
#     unknown_preds = pred[unknown_mask]
#     unknown_probs = probs[unknown_mask]
#
#     # Visualize prediction distribution
#     plt.hist(unknown_preds.cpu().numpy(), bins=2, edgecolor='black')
#     plt.title("Prediction Distribution for Unknown Nodes")
#     plt.xlabel("Predicted Label")
#     plt.ylabel("Count")
#     plt.savefig('unknown_pred_distribution.png')
#     plt.close()
#
#     # Save results
#     result_df = pd.DataFrame({
#         'NodeIndex': unknown_indices.cpu().numpy(),
#         'PredictedLabel': unknown_preds.cpu().numpy(),
#         'Prob_0': unknown_probs[:, 0].cpu().numpy(),
#         'Prob_1': unknown_probs[:, 1].cpu().numpy()
#     })
#     result_df.to_csv('unknown_predictions_tgat.csv', index=False)
#     logging.info("Predictions saved to 'unknown_predictions_tgat.csv'")
#     print("Predictions saved to 'unknown_predictions_tgat.csv'")
#
# # Plot learning curves
# plt.plot(train_losses, label='Train Loss')
# plt.plot(val_accs, label='Val Accuracy')
# plt.plot([i * 10 for i in range(len(test_accs))], test_accs, label='Test Accuracy')
# plt.legend()
# plt.title("Learning Curves")
# plt.xlabel("Epoch")
# plt.ylabel("Value")
# plt.savefig('learning_curve_tgat.png')
# plt.close()
#
# # Check prediction distribution
# print("Known nodes label distribution:", data.y[train_mask | test_mask].bincount().cpu().numpy())
# print("Unknown nodes prediction distribution:", unknown_preds.bincount().cpu().numpy())


import torch
import random
import numpy as np
import logging
from torch_geometric.datasets import EllipticBitcoinDataset
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GATConv
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, precision_score, recall_score
import pandas as pd
import matplotlib.pyplot as plt

# Set random seed
seed = 42
torch.manual_seed(seed)
np.random.seed(seed)
random.seed(seed)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(seed)

# Logging configuration
logging.basicConfig(filename='tgat_training.log', level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Hyperparameters
config = {
    'in_channels': 165,
    'hidden_channels': 64,
    'out_channels': 2,
    'lr': 0.01,
    'weight_decay': 5e-4,
    'epochs': 200,
    'patience': 20,
    'dropout': 0.3,
    'num_heads': 4,
}

# Temporal Encoding
class TemporalEncoding(nn.Module):
    def __init__(self, d_model):
        super(TemporalEncoding, self).__init__()
        self.d_model = d_model
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))
        self.register_buffer('div_term', div_term)

    def forward(self, t):
        pe = torch.zeros(t.size(0), self.d_model, device=t.device)
        t = t.unsqueeze(-1)
        pe[:, 0::2] = torch.sin(t * self.div_term)
        pe[:, 1::2] = torch.cos(t * self.div_term)
        return pe

# Custom Temporal GAT Convolution
class CustomTemporalGATConv(nn.Module):
    def __init__(self, in_channels, out_channels, heads=1, dropout=0.0):
        super(CustomTemporalGATConv, self).__init__()
        self.temporal_encoder = TemporalEncoding(out_channels)
        self.gat = GATConv(in_channels + out_channels, out_channels, heads=heads, dropout=dropout)
        self.dropout = dropout

    def forward(self, x, edge_index, edge_time):
        time_emb = self.temporal_encoder(edge_time)  # Shape: [num_edges, out_channels]
        num_nodes = x.size(0)
        edge_time_emb = torch.zeros(num_nodes, time_emb.size(1), device=x.device)
        row, col = edge_index
        for i in range(num_nodes):
            mask = (col == i)
            if mask.sum() > 0:
                edge_time_emb[i] = time_emb[mask].mean(dim=0)
        x = torch.cat([x, edge_time_emb], dim=-1)
        x = self.gat(x, edge_index)
        return x

# Load dataset
dataset = EllipticBitcoinDataset(root='data/EllipticBitcoinDataset')
data = dataset[0]

# Device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
logging.info(f"Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}")
data = data.to(device)

# Check dataset integrity
if data.x is None or data.edge_index is None or data.y is None:
    raise ValueError("Invalid dataset: missing features, edges, or labels")

# Feature normalization
scaler = StandardScaler()
data.x = torch.tensor(scaler.fit_transform(data.x.cpu()), dtype=torch.float).to(device)

# Process timestamps
if hasattr(data, 'edge_attr') and data.edge_attr is not None:
    edge_time = data.edge_attr[:, 0].clone().to(device)
    edge_time = (edge_time - edge_time.min()) / (edge_time.max() - edge_time.min() + 1e-6)
else:
    edge_time = torch.arange(data.edge_index.shape[1], dtype=torch.float, device=device)
    edge_time = edge_time / (edge_time.max() + 1e-6)
logging.info(f"Edge time shape: {edge_time.shape}")

# Data distribution
known_indices = (data.y != 2).nonzero(as_tuple=True)[0]
num_known_nodes = known_indices.shape[0]
logging.info(f"Number of known nodes: {num_known_nodes}")
print(f"Number of known nodes: {num_known_nodes}")
print("Label distribution in known nodes:", data.y[known_indices].bincount().cpu().numpy())

# Train/Val/Test split
indices = torch.randperm(num_known_nodes)
train_size = int(num_known_nodes * 0.6)
val_size = int(num_known_nodes * 0.2)
train_indices = known_indices[indices[:train_size]]
val_indices = known_indices[indices[train_size:train_size + val_size]]
test_indices = known_indices[indices[train_size + val_size:]]

train_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
val_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
test_mask = torch.zeros(data.num_nodes, dtype=torch.bool, device=device)
train_mask[train_indices] = True
val_mask[val_indices] = True
test_mask[test_indices] = True
logging.info(
    f"Train mask shape: {train_mask.shape}, Val mask shape: {val_mask.shape}, Test mask shape: {test_mask.shape}")

# Define TGAT model
class TGAT(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_heads, dropout):
        super(TGAT, self).__init__()
        self.conv1 = CustomTemporalGATConv(in_channels, hidden_channels, heads=num_heads, dropout=dropout)
        self.bn1 = torch.nn.BatchNorm1d(hidden_channels * num_heads)
        self.conv2 = CustomTemporalGATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, dropout=dropout)
        self.bn2 = torch.nn.BatchNorm1d(hidden_channels * num_heads)
        self.conv3 = CustomTemporalGATConv(hidden_channels * num_heads, out_channels, heads=1, dropout=dropout)
        self.dropout = torch.nn.Dropout(dropout)

    def forward(self, x, edge_index, edge_time):
        x = self.conv1(x, edge_index, edge_time)
        x = self.bn1(x)
        x = F.relu(x)
        x = x = self.dropout(x)  # ✅正确
        x = self.conv2(x, edge_index, edge_time)
        x = self.bn2(x)
        x = F.relu(x)
        x = x = self.dropout(x)  # ✅正确g)
        x = self.conv3(x, edge_index, edge_time)
        return F.log_softmax(x, dim=1)

# Initialize model and optimizer
model = TGAT(
    in_channels=config['in_channels'],
    hidden_channels=config['hidden_channels'],
    out_channels=config['out_channels'],
    num_heads=config['num_heads'],
    dropout=config['dropout']
).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)

# Training and evaluation functions
def train():
    model.train()
    optimizer.zero_grad()
    out = model(data.x, data.edge_index, edge_time)
    loss = F.nll_loss(out[train_mask], data.y[train_mask])
    loss.backward()
    optimizer.step()
    return loss.item()

def evaluate(mask):
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index, edge_time)
        pred = out.argmax(dim=1)
        correct = (pred[mask] == data.y[mask]).sum()
        acc = correct.float() / mask.sum().float()
        pred_cpu = pred[mask].cpu().numpy()
        true_cpu = data.y[mask].cpu().numpy()
        try:
            f1 = f1_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
            precision = precision_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
            recall = recall_score(true_cpu, pred_cpu, average='weighted', zero_division=0)
        except Exception as e:
            logging.warning(f"Evaluation metrics warning: {e}")
            f1, precision, recall = 0.0, 0.0, 0.0
    return acc.item(), f1, precision, recall

# Training loop
best_val_acc = 0
counter = 0
train_losses, val_accs, test_accs = [], [], []

for epoch in range(config['epochs']):
    loss = train()
    val_acc, val_f1, val_precision, val_recall = evaluate(val_mask)
    scheduler.step(loss)

    train_losses.append(loss)
    val_accs.append(val_acc)

    if epoch % 10 == 0:
        test_acc, test_f1, test_precision, test_recall = evaluate(test_mask)
        test_accs.append(test_acc)
        logging.info(f'Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, '
                     f'F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
        print(f'Epoch {epoch}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}, '
              f'F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        counter = 0
        torch.save(model.state_dict(), 'best_tgat_model.pth')
    else:
        counter += 1
        if counter >= config['patience']:
            logging.info("Early stopping triggered")
            print("Early stopping triggered")
            break

# Load best model
model.load_state_dict(torch.load('best_tgat_model.pth'))

# Final evaluation
test_acc, test_f1, test_precision, test_recall = evaluate(test_mask)
logging.info(
    f'Final Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')
print(f'Final Test Acc: {test_acc:.4f}, F1: {test_f1:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}')

# Inference on unknown nodes
model.eval()
with torch.no_grad():
    out = model(data.x, data.edge_index, edge_time)
    pred = out.argmax(dim=1)
    probs = F.softmax(out, dim=1)
    unknown_mask = (data.y == 2)
    unknown_indices = unknown_mask.nonzero(as_tuple=True)[0]
    unknown_preds = pred[unknown_mask]
    unknown_probs = probs[unknown_mask]

    # Visualize prediction distribution
    plt.hist(unknown_preds.cpu().numpy(), bins=2, edgecolor='black')
    plt.title("Prediction Distribution for Unknown Nodes")
    plt.xlabel("Predicted Label")
    plt.ylabel("Count")
    plt.savefig('unknown_pred_distribution.png')
    plt.close()

    # Save results
    result_df = pd.DataFrame({
        'NodeIndex': unknown_indices.cpu().numpy(),
        'PredictedLabel': unknown_preds.cpu().numpy(),
        'Prob_0': unknown_probs[:, 0].cpu().numpy(),
        'Prob_1': unknown_probs[:, 1].cpu().numpy()
    })
    result_df.to_csv('unknown_predictions_tgat.csv', index=False)
    logging.info("Predictions saved to 'unknown_predictions_tgat.csv'")
    print("Predictions saved to 'unknown_predictions_tgat.csv'")

# Plot learning curves
plt.plot(train_losses, label='Train Loss')
plt.plot(val_accs, label='Val Accuracy')
plt.plot([i * 10 for i in range(len(test_accs))], test_accs, label='Test Accuracy')
plt.legend()
plt.title("Learning Curves")
plt.xlabel("Epoch")
plt.ylabel("Value")
plt.savefig('learning_curve_tgat.png')
plt.close()

# Check prediction distribution
print("Known nodes label distribution:", data.y[train_mask | test_mask].bincount().cpu().numpy())
print("Unknown nodes prediction distribution:", unknown_preds.bincount().cpu().numpy())
